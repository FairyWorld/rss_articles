## 本周话题：大数据已死

“大数据”曾经十分火爆，全社会都在使用这个词，认为它会改变世界。

![](https://cdn.beekka.com/blogimg/asset/202302/bg2023022203.webp)

[百度指数](https://index.baidu.com/v2/main/index.html#/trend/%E5%A4%A7%E6%95%B0%E6%8D%AE?words=%E5%A4%A7%E6%95%B0%E6%8D%AE)显示，“大数据”从2011年开始进入搜索引擎，然后快速传播，在2017年～2019年之间达到顶峰。

那时，企业纷纷寻求大数据解决方案，出现了很多相关的招聘岗位，还都是高薪。国内高校更是纷纷开设[“大数据学院”](https://m.mp.oeeee.com/a/BAAFRD000020200728349302.html)。

![](https://cdn.beekka.com/blogimg/asset/202302/bg2023022204.webp)

当时，大家普遍相信，海量数据的处理和分析能力，决定了一个企业甚至一个国家在信息时代的竞争力。专家预言“世界将被指数式增长的数据淹没”。

但是，十年过去了，这件事似乎并没有发生，我们依然足以处理所产生的数据。**预言中的大数据时代，看上去不仅没有来临，反而变得更遥远了。** 大家有没有这种感觉：大数据的热度在降温，被提及次数少了，招聘岗位也慢慢不见了。

与之相应的是，这十年间，大数据这个技术领域，进展得也不好：没有诞生新的概念和理论，技术没有突破，很多方向都停滞不前，进展甚微。

比如，NoSQL 数据库的设计目的之一，就是为了处理大数据。但是这几年，它的声势越来越小，陷入停滞，反而是传统的关系型数据库（SQLite、Postgres、MySQL）强劲增长，越来越受欢迎。

最近，谷歌的大数据工程师 Jordan Tigani 甚至直言不讳地说：“[大数据已死](https://motherduck.com/blog/big-data-is-dead/)”。

他认为：**大数据时代已经结束了，大数据的存储和分析，作为一个技术问题已经解决了。** 用户不必担心数据大小了，再多的数据都不是问题。

他的理由很有说服力，下面就跟大家分享。

<u>（1）大多数企业没有那么多数据。</u> 绝大多数企业的数据量，不到 1TB，很多甚至不到 100GB。这意味着，绝大多数公司遇不到大数据瓶颈。

举例来说，一家中等规模的制造业公司，假设拥有1000名客户，每名客户每天下一个订单，每个订单包含100个产品。那么，这家公司一天产生的数据量，依然远远小于 1 MB，三年后也只有 1 GB，达到 1 TB 的数据需要几千年。

就算是大型互联网公司，大多数时候也到不了大数据的门槛。假设某个营销活动有100万用户参加，并且同一时间在开展几十个这样的营销活动，数据量依然不足 1 GB，就算加上各种日志，可能也只有几个 GB，这跟大数据相差甚远。

<u>（2）存储和计算正在分离。</u>  大数据包含“数据存储”和“数据计算”两个方面，如果放在一个系统里面处理，确实很难。

但是，这两方面其实是两个独立的系统，现在已经能够做到彼此脱钩，各自都可以独立扩展。这意味着，“数据计算”不受“数据存储”（数据库大小）的限制，反之依然。

因此，大数据这个问题就不存在了，变成了海量存储和大型计算两个问题。

<u>（3）没有新业务的情况下，数据是线性增长的。</u> 所谓“线性增长”，可以理解成，每天的新增数据与前一天的数据，结构相同。

大多数时候，以前的数据不会发生变化，计算需求也不会改变太多，这时只要对最近的新增数据进行分析，然后保存，就可以了。你很少需要每天扫描旧数据：那些数据一成不变，为什么要一遍一遍处理它们呢？

因此，对于一家企业来说，数据会指数式增长，这个假设并不成立。现实生活中，数据计算的需求，其实比数据存储的需求小得多，原因就在于老数据很少需要再次计算。

<u>（4）人们感兴趣的，往往只是一小部分最近的数据</u>。最频繁的查询是针对24小时内产生的数据，一周前的数据的查询可能性要低20倍，一个月前的历史数据只会偶尔被查询。

这意味着，大数据更像静态数据，而不完全是动态数据，数据存储和数据计算都更容易管理。一个包含10年数据的表格，可能会达到 PB 级别，但是如果很少访问当天以前的数据，那部分数据就可以压缩保存，整个数据表压缩后可能不到 50 GB。

<u>（5）海量数据的查询非常少。</u>真正拥有大数据的客户，几乎从不查询全部数据。他们90%的查询涉及的数据少于 100 MB，涉及 TB 级别数据的查询非常少。

而且，就算查询 TB 级别数据，查询性能的优先级往往并不高，客户愿意等一个周末或几天才拿到结果。

另外，大型数据集的查询非常昂贵，客户也不愿意多使用。谷歌的 BigQuery 的 PB 级别查询报价是 5,000 美元，即使是大公司也不会经常使用。

<u>（6）硬件的飞速发展，使得单台计算机的计算能力大增。</u> 2004年，谷歌发表 MapReduce 论文时，行业的计算能力还比较弱，很多计算必须通过分布式完成。

2006年，AWS 推出了 EC2 云主机，你只能用到一个单核 CPU 和 2 GB 内存。今天，AWS 的标准实例具有64个内核和 256 GB 内存。如果愿意多花钱，还可以拿到445个内核和超过 24 TB 内存。

单台计算机的计算能力大大增强，意味着大数据的最大难点——分布式计算——即使被用到，困难程度也大大降低。

综合以上六点理由，可以得出结论：数据量的大小，已经不需要特别关注了，再也不必担心处理不了海量数据了。大数据作为一个技术问题，已经解决了。
