## 大数据已死

“大数据”曾经是一个热门概念，10年前曾经风靡一时，业界都是关于它的议论，高校纷纷开设“大数据学院”，大家相信海量数据的处理和分析能力，决定了一个企业甚至一个国家的前途。

10年前曾经有预言：“世界将被指数式增长的数据淹没”，处理数据的旧方法行不通了，新生成的数据将让现有系统无法应付，任何找到新的数据处理方法的人将超越他们的竞争对手。

但是这件事并没有发生，看起来，我们依然足以处理产生的数据。

这十年间，大数据这个领域，发展得并不好，没有诞生新的概念和理论，技术没有突破，很多方向都停滞不前，进展甚微。

传统的关系型数据（SQLite、Postgres、MySQL）都在强劲增长，而专门为大数据设计的 NoSQL 数据库甚至 NewSQL 数据库则停滞不前。

最近，谷歌的大数据工程师 JORDAN TIGANI 甚至直言不讳地说：“[大数据已死](https://motherduck.com/blog/big-data-is-dead/)”。

我看到这句话，感到大惑不解：海量数据明明是一个现实的问题，为什么这个领域已经死了呢？

我仔细读了他的长文，才明白他的意思是：大数据时代已经结束了，大数据作为一个技术问题已经解决了，现在我们不用担心数据大小了，专注于其他方面就可以了。

我感到他的理由确实有道理，下面就跟大家分享。

（1）大多数企业没有那么多数据。

绝大多数企业的数据量，不到1TB，很多甚至不到100GB。这意味着，绝大多数公司遇不到大数据瓶颈。

一家中型制造业公司，大概拥有1000名客户，假设每位客户每天下一个订单，每个订单包含100个产品，一天产生的数据依然远远小于 1 MB，三年后你也只有 1 GB，产生 1 TB 的数据需要几千年。

再看一个更大的例子，假设某个营销活动有100万个用户参加，你同时开展了几十个这样的营销活动，你的数据库依然不到 1 GB，就算加上各种日志，可能也只有几个 GB，怎么都不到大数据的程度。

（2）存储和计算正在分离。

数据存储和数据计算，是两个独立的系统，彼此之间是脱钩的，都可以独立扩展。

这意味着，数据处理不受数据库大小的限制，反之依然。大数据这个问题因此就不存在了，变成了数据存储和大型计算这两个问题。

（3）没有新业务的情况下，数据是线性增长的。每天都有新的数据进来，加入以前的数据。所以，数据存储也是线性增长的。

大多数情况下，以前的数据不会发生变化的，计算需求也不会改变太多。只要对最近的数据进行分析就可以了，然后保存，很少需要每天扫描旧数据：那些数据一成不变，为什么要一遍一遍处理它们呢？

现实生活中，数据计算的需求，其实比数据存储的需求小得多，原因就在于老数据很少需要再次计算。

因此，对于一家企业来说，数据会指数式增长，这个假设并不成立。

（4）数据的静态特征。

大数据不是完全动态的，更像静态数据。人们感兴趣的往往是其中的一小部分数据，比如，人们查看前一小时、前一天或上周的数据。新近的数据和一些较小的表，往往会被更频繁地查询，而大型数据表只会被有选择性地查询。

最多的查询是针对24小时内产生的数据，一周前的数据比当天数据的查询可能低20倍，一个月前的历史数据只会偶尔被查询。

数据的静止性，意味着数据存储的大小更容易管理。一个包含10年数据的 PB 级表，如果很少访问当天以前的数据，那么这个数据表可以这些数据可能压缩了不到 50 GB。

（5）工作负载往往小于整体数据。

数据量巨大的客户几乎从不查询海量数据。90% 的查询处理的数据少于 100 MB，TB 级数据的查询非常少。

而且，就算查询 TB 级数据，查询性能的优先级往往并不高，客户愿意等一个周末或几天才拿到结果。

另外，大型数据集的查询非常昂贵，客户也不愿意多使用。谷歌的 BigQuery 的 PB 查询报价是 5,000 美元，即使是大公司也不会经常使用。

（6）硬件的飞速发展，使得单台机器的计算能力大增。

2004 年，谷歌发布 MapReduce 论文时，很多数据计算必须使用分布式节点完成。2006年，AWS 推出了 EC2，你可以获得的实例大小只有单核和 2 GB RAM，今天 AWS 的标准实例使用具有 64 个内核和 256 GB RAM 的物理服务器，愿意多花钱，还可以拿到超过 24TB 的 RAM 或 445 个 CPU 内核？

单台计算机能够完成的数据计算能力大大增强，大数据最大难点的分布式计算，也就被大大减轻了。

综上所述，大数据是真实存在的，但是作为一个技术问题，已经解决了。我们再也不必担心，处理不了海量数据，被其淹没了。
