## 训练材料用完之日

将历史趋势预测到未来可能会产生误导，因为这种趋势得到了过去十年计算量异常大幅增长的支持。

AI 发展得这么快，训练材料会不会用完？

> 根据预测，到 2030 年到 2050 年，我们将耗尽所有言数据的存量，到 2026 年之前耗尽普通的语言数据的存量（已经整理好的训练集），到 2030 年到 2060 年耗尽视觉数据存量的存量。这可能会减慢机器学习的进展。


https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset

作者承认，这个研究依然于很多假设，有极大的局限性。但是 数据集的增长速度远快于数据存量，因此如果当前趋势继续下去，耗尽数据存量是不可避免的。

我们认为，到 2040 年，由于缺乏训练数据，ML 模型的扩展（在训练计算中衡量）有大约 20% 2的可能性会显着放缓。

接下来，我们就不知道 AI 会怎么发展了，因为它没有更多的人类材料可以训练了。接下来，AI 就不能用概率去预测，而要自己琢磨了该怎么解决问题了。

这是有生之年，我们都能看到的，我不禁有些好奇，很难想象，我们这一代人看到了 AI 的崛起，还会看到 AI 的停滞吗？

---

定义数据集是否足够的最常用方法是应用 10 倍规则。该规则意味着输入数据量（即示例数）应该是模型具有的自由度数的十倍。通常，自由度是指数据集中的参数。 

因此，例如，如果您的算法根据 1,000 个参数区分猫的图像和狗的图像，则需要 10,000 张图片来训练模型。 

虽然机器学习中的 10 倍法则很流行，但它只适用于小模型。较大的模型不遵循此规则，因为收集的示例数量不一定反映训练数据的实际数量。

---

![](https://www.stylefactoryproductions.com/wp-content/uploads/2023/04/chatgpt-4-training-data-size.png)

用于训练 ChatGPT-4（最新版本的 ChatGPT）的数据集估计包含100 万亿个参数，比 ChatGPT-3 的训练数据大 5 倍以上。（来源：有线和OpenAI。）
ChatGPT 前四个版本的数据集大小。
ChatGPT 前四个版本的数据集大小（来源：数据营和有线.)
Chat-GPT-4 训练数据包括来自 ChatGPT-3 用户的反馈以及来自 50 多位 AI 安全专家的反馈。（来源：OpenAI。）
ChatGPT-3 的数据集包含来自 5 个来源的文本数据，每个来源具有不同的比例权重。（来源：OpenAI。）
ChatGPT-3 数据集的60%基于所谓的“普通爬网”数据的过滤版本，其中包括网页数据、元数据提取物和来自 8 年多网络爬虫的文本提取物。（来源：OpenAI。）
ChatGPT-3 数据集的22%来自“WebText2”，它由获得三个或更多赞成票的 Reddit 帖子组成。（来源：OpenAI。）
ChatGPT-3 数据集的16%来自两个基于互联网的图书收藏。这些书籍包括小说、非小说以及范围广泛的学术文章。（来源：OpenAI。）
ChatGPT-3 数据集的3%来自英文版的维基百科。（来源：OpenAI。）
ChatGPT-3 93%的数据集是英文的（来源：OpenAI。）

--- 

https://www.sciencefocus.com/future-technology/gpt-3/

该模型是使用来自互联网的文本数据库进行训练的。这包括从书籍、网络文本、维基百科、文章和互联网上的其他文章中获得的高达 570GB 的数据。更准确地说，系统输入了 3000 亿个单词。

-- Behind ChatGPT's Wisdom: 300 Bn Words, 570 GB Data

---

https://finance.yahoo.com/news/openai-sam-altman-says-giant-164924270.html

奥特曼上周在麻省理工学院的一次活动中表示：“我认为我们正处于将成为这些巨型模型的时代的末尾，我们将以其他方式让它们变得更好。” 他陈述的理由是，最好关注“快速增加的能力”而不是参数数量，如果可以通过减少参数数量或同时利用多个较小的模型来实现能力改进，那就太好了。

正如 VentureBeat指出的那样，奥特曼的想法背后可能有成本驱动因素。法学硕士真的非常昂贵——据报道， GPT-4 的培训花费了 1 亿美元。