## 本周话题：训练材料用完之日

现在的新闻报道，天天有 AI 的新闻，里面会提到很多模型。

分辨模型的强弱，有一个关键指标，就是看它有多少个参数。一般来说，参数的数量越多，模型就越强。

GPT-2 有15亿个参数，GPT-3 和 ChatGPT 有[1750亿个](https://developer.nvidia.com/zh-cn/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/)，GPT-4 没有公布这个指标，据传比上一代大5倍以上。

![](https://cdn.beekka.com/blogimg/asset/202305/bg2023050303.webp)

那么，什么是参数呢？

按照我粗浅的理解，参数相当于模型预测时，所依据的神经网络的节点数量。**参数越多，就代表了模型所考虑的各种可能性越多，计算量越大，效果越好。**

既然参数越多越好，那么参数会无限增长吗？

答案是不会的，因为参数受到训练材料的制约。必需有足够的训练材料，才能计算出这些参数，**如果参数无限增长，训练材料势必也要无限增长。**

我看到的一种说法是，训练材料至少应该是参数的10倍。举例来说，一个区分猫照片和狗照片的模型，假定有1,000个参数，那么至少应该用10,000张图片来训练。 

![](https://cdn.beekka.com/blogimg/asset/202305/bg2023050305.webp)

ChatGPT 有1750亿个参数，那么训练材料最好不少于17500亿个词元（token）。“词元”就是各种单词和符号，以小说《红楼梦》为例，它有788,451字，就算100万个词元。那么， ChatGPT 的训练材料相当于175万本《红楼梦》。

根据[报道](https://www.sciencefocus.com/future-technology/gpt-3/)，ChatGPT 实际上用了 570 GB 的训练材料，来自维基百科、互联网图书馆、Reddit 论坛、推特等等。

![](https://cdn.beekka.com/blogimg/asset/202305/bg2023050306.webp)

大家想一想，更强大的模型需要更多的训练材料，**问题是能找到这么多材料吗，会不会材料有一天不够用？**

我告诉大家，真的有学者写过[论文](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset)，研究这个问题。

过去10年来，AI 训练数据集的增长速度远快于全世界的数据存量的增长速度。如果这种趋势继续下去，耗尽数据存量是不可避免的。

论文给出了三个时间点。

> - 2026年：用完一般的语言数据
> - 2030年～2050年：用完所有的语言数据
> - 2030年～2060年：用完所有的视觉数据

也就是说，根据他们的预测，**大概三四年后，新的训练材料就会很难找**。最迟三十年后，全世界所有材料都不够 AI 的训练。

![](https://cdn.beekka.com/blogimg/asset/202305/bg2023050307.webp)

上图是作者给的趋势图，虚线是训练材料的增长速度，红线和蓝线是模型增长速度的不同预测。到了2035年以后，这三根线就合在一起了，曲线变得越来越平。

作者认为，到了那时，由于没有足够的训练材料，AI 模型的发展速度可能就会显著放缓。

如果他的预测是正确的，就意味着，跟大家想的不一样，**AI 飞速发展不会持续很久。现在也许就是发展最快的阶段**，然后就会开始放慢，等到本世纪中叶就会显著放慢，接近停滞，跟量子物理学的现状差不多。

---

作者承认，这个研究依然于很多假设，有极大的局限性。但是 数据集的增长速度远快于数据存量，因此如果当前趋势继续下去，耗尽数据存量是不可避免的。

我们认为，到 2040 年，由于缺乏训练数据，ML 模型的扩展（在训练计算中衡量）有大约 20% 2的可能性会显着放缓。

接下来，我们就不知道 AI 会怎么发展了，因为它没有更多的人类材料可以训练了。接下来，AI 就不能用概率去预测，而要自己琢磨了该怎么解决问题了。

这是有生之年，我们都能看到的，我不禁有些好奇，很难想象，我们这一代人看到了 AI 的崛起，还会看到 AI 的停滞吗？

---

定义数据集是否足够的最常用方法是应用 10 倍规则。该规则意味着输入数据量（即示例数）应该是模型具有的自由度数的十倍。通常，自由度是指数据集中的参数。 

因此，例如，如果您的算法根据 1,000 个参数区分猫的图像和狗的图像，则需要 10,000 张图片来训练模型。 

虽然机器学习中的 10 倍法则很流行，但它只适用于小模型。较大的模型不遵循此规则，因为收集的示例数量不一定反映训练数据的实际数量。

---

![](https://www.stylefactoryproductions.com/wp-content/uploads/2023/04/chatgpt-4-training-data-size.png)

用于训练 ChatGPT-4（最新版本的 ChatGPT）的数据集估计包含100 万亿个参数，比 ChatGPT-3 的训练数据大 5 倍以上。（来源：有线和OpenAI。）
ChatGPT 前四个版本的数据集大小。
ChatGPT 前四个版本的数据集大小（来源：数据营和有线.)
Chat-GPT-4 训练数据包括来自 ChatGPT-3 用户的反馈以及来自 50 多位 AI 安全专家的反馈。（来源：OpenAI。）
ChatGPT-3 的数据集包含来自 5 个来源的文本数据，每个来源具有不同的比例权重。（来源：OpenAI。）
ChatGPT-3 数据集的60%基于所谓的“普通爬网”数据的过滤版本，其中包括网页数据、元数据提取物和来自 8 年多网络爬虫的文本提取物。（来源：OpenAI。）
ChatGPT-3 数据集的22%来自“WebText2”，它由获得三个或更多赞成票的 Reddit 帖子组成。（来源：OpenAI。）
ChatGPT-3 数据集的16%来自两个基于互联网的图书收藏。这些书籍包括小说、非小说以及范围广泛的学术文章。（来源：OpenAI。）
ChatGPT-3 数据集的3%来自英文版的维基百科。（来源：OpenAI。）
ChatGPT-3 93%的数据集是英文的（来源：OpenAI。）

--- 

https://www.sciencefocus.com/future-technology/gpt-3/

该模型是使用来自互联网的文本数据库进行训练的。这包括从书籍、网络文本、维基百科、文章和互联网上的其他文章中获得的高达 570GB 的数据。更准确地说，系统输入了 3000 亿个单词。

-- Behind ChatGPT's Wisdom: 300 Bn Words, 570 GB Data

---

https://finance.yahoo.com/news/openai-sam-altman-says-giant-164924270.html

奥特曼上周在麻省理工学院的一次活动中表示：“我认为我们正处于将成为这些巨型模型的时代的末尾，我们将以其他方式让它们变得更好。” 他陈述的理由是，最好关注“快速增加的能力”而不是参数数量，如果可以通过减少参数数量或同时利用多个较小的模型来实现能力改进，那就太好了。

正如 VentureBeat指出的那样，奥特曼的想法背后可能有成本驱动因素。法学硕士真的非常昂贵——据报道， GPT-4 的培训花费了 1 亿美元。