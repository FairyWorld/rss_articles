## 李开复梳理人工智能

前些日子，我读了李开复老师的两本书：《AI·未来》（浙江人民出版社，2018）和《AI·未来进行式》（浙江人民出版社，2022）。

![](https://cdn.beekka.com/blogimg/asset/202412/bg2024121701.webp)

![](https://cdn.beekka.com/blogimg/asset/202412/bg2024121702.webp)

这两本书都是面向社会大众解释 AI，不是技术类书籍，但是有一些段落，阐述 AI 概念。

李开复老师用通俗的语言来解释，人工智能、机器学习、神经网络、深度学习……**这些词到底是什么意思？彼此有什么关系？**

我觉得，对理解 AI 的体系，挺有启发的。他的解释分散在原文各个章节，我将它们整理在一起。

需要说明的是，为了行文连贯，下面的解释没有完全照搬原文，而是用我的语言改写了。

**（1）人工智能**

1956年夏天，计算机科学家约翰·麦卡锡（John McCarthy）首次提出了“人工智能”（AI）这个概念。

<u>它指的是，通过软件和硬件，来完成通常需要人类智能才能完成的任务。</u>它的研究对象，就是在机器上模拟人类智能。

**（2）机器学习**

早期，人工智能研究分成两个阵营。

第一个阵营是规则式（rule-based）方法，又称专家系统（expert systems），指的是人类写好一系列逻辑规则，来教导计算机如何思考。

可想而知，对于复杂的、大规模的现实问题，很难写出完备的、明确的规则。所以，这种方法的进展一直很有限。

第二个阵营就是<u>机器学习（machine learning），指的是没有预置的规则，只是把材料提供给计算机，让机器通过自我学习，自己发现规则，给出结果。</u>

**（3）神经网络**

神经网络（neural network）是机器学习的一种主要形式。

<u>它的思路就是在机器上模拟人脑的结构，构建类似生物神经元的网络来处理信息。</u>

一个计算节点就是一个神经元，大量的计算节点组成网络，进行协同计算。

神经网络需要极大的算力，以及海量的训练材料。以前，这是难以做到的，所以20世纪70年代开始，就陷入了停滞，长期没有进展。

**（4）深度学习**

深度学习是神经网络的一种实现方法，在20世纪80年代杰弗里·辛顿提出。它让神经网络研究重新复活。

<u>它的核心就是找到了一种方法，让多层神经元可以进行有效计算，大大提高了神经网络的性能。“深度学习”这个名字，就是比喻多层神经元的自主学习过程。</u>

多层神经元包括一个输入层和一个输出层，它们之间有很多中间层（又称隐藏层）。以前，计算机算力有限，只能支撑一两个中间层，深度学习使得我们可以构建成千上万个中间层的网络，具有极大的“深度”。

**（5）Transformer**

早些年，深度学习用到的方法是卷积神经网络（CNN）和循环神经网络（RNN）。

2017年，谷歌的研究人员发明了一种新的深度学习处理方法，叫做 Transformer（转换器）。

<u>它不同于以前的方法，不再一个个处理输入的单词，而是一次性处理整个输入，对每个词分配不同的权重。</u>

这种方法直接导致了2022年 ChatGPT 和后来无数生成式 AI 模型的诞生，是神经网络和深度学习目前的主流方法。

由于基于 Transformer 的模型需要一次性处理整个输入，所以都有“上下文大小”这个指标，指的是一次可以处理的最大输入。

比如，GPT-4 Turbo 的上下文是 128k 个 Token，相当于一次性读取超过300页的文本。上下文越大，模型能够考虑的信息就越多，生成的回答也就越相关和连贯，相应地，所需要的算力也就越多。
